# -*- coding: utf-8 -*-
"""Копия блокнота "Untitled11.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l7qBnGXCfEkCIKpdS_Fgk66QNWnMQJTy
"""

# Длительность аниме в месяцах (если обе даты известны)
df_clean['Duration_months'] = (
    (df_clean['End_Date'].dt.year - df_clean['Start_Date'].dt.year) * 12 +
    (df_clean['End_Date'].dt.month - df_clean['Start_Date'].dt.month)
)

# Категория по Score
df_clean['Score_group'] = pd.cut(
    df_clean['Score'],
    bins=[0, 7, 8, 9, 10],
    labels=['Low', 'Medium', 'High', 'Top']
)
df_clean

import requests
from bs4 import BeautifulSoup
import time
import pandas as pd

headers={"User-Agent": "Mozilla/5.0"}

import os

titles = []
scores = []
anime_type_list = []
episodes_list = []
start_date_list = []
end_date_list = []
members_list = []

for limit in range(0, 10000, 50):
    url = f"https://myanimelist.net/topanime.php?limit={limit}&ajax=1"
    print("Parsing:", url)

    r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
    soup = BeautifulSoup(r.text, "html.parser")

    rows = soup.find_all("tr", class_="ranking-list")

    if not rows:
        print("N/A:", limit)
        continue

    for row in rows:

        title = row.find("h3").text.strip()
        titles.append(title)

        score_tag = row.select_one("td.score span")
        scores.append(score_tag.text.strip() if score_tag else "N/A")

        info = row.find("div", class_="information")
        if info:
            lines = [l.strip() for l in info.text.split("\n") if l.strip()]
        else:
            lines = ["N/A", "N/A", "N/A"]

        # TYPE + EPISODES
        type_ep = lines[0] if len(lines) > 0 else "N/A"
        if "(" in type_ep:
            anime_type = type_ep.split("(")[0].strip()
            eps = type_ep.split("(")[1].replace("eps)", "").replace("ep)", "").strip()
        else:
            anime_type = type_ep
            eps = "N/A"
        anime_type_list.append(anime_type)
        episodes_list.append(eps)

        # AIR DATES
        date_line = lines[1] if len(lines) > 1 else "N/A"

        date_parts = [part.strip() for part in date_line.split(" - ")]

        current_start_date = "N/A"
        current_end_date = "N/A"

        if len(date_parts) == 2:
            current_start_date = date_parts[0]
            current_end_date = date_parts[1]
        elif len(date_parts) == 1:
            current_start_date = date_parts[0]
            current_end_date = "Ongoing"
        # If len(date_parts) is 0 or >2, they remain "N/A" as initialized

        start_date_list.append(current_start_date)
        end_date_list.append(current_end_date)

        # MEMBERS
        members_line = lines[2] if len(lines) > 2 else "N/A"
        members_list.append(members_line)

    time.sleep(1)

# CREATE DATAFRAME
df = pd.DataFrame({
    "Title": titles,
    "Score": scores,
    "Type": anime_type_list,
    "Episodes": episodes_list,
    "Start_Date": start_date_list,
    "End_Date": end_date_list,
    "Members": members_list
})

#  CSV
filename = "Anime_top_10000.csv"
df.to_csv(filename, index=False, encoding="utf-8-sig")
print("Ready ", len(df))


if os.path.exists(filename):
    print(f"CSV file found: {filename}. Loading data...")
    df = pd.read_csv(filename)
else:
    print("CSV file not found. Using the DataFrame just scraped.")

# first 5 and last 5
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 120)

print("\n--- FIRST 5 ROWS ---")
print(df.head())

print("\n--- LAST 5 ROWS ---")
print(df.tail())

from google.colab import drive
drive.mount('/content/drive')

"""# **Part 2 — Data Cleaning & Preparation**

# 1. Data inspection

a. head(), df.info(), df.describe()
"""

import pandas as pd
import numpy as np
df = pd.read_csv("Anime_top_10000.csv")
df

df.head()

df.describe()

df.info()

"""b. Identify missing values, wrong types, duplicates"""

#  missing values problem
print(df.isnull().sum())

# Wrong value problem
print(df.dtypes)
# Start_date, End_date, Episodes, (members) wrong types

# Duplicates problem
print("\n=== Дубликаты ===")
print("Количество дубликатов:", df.duplicated().sum())
duplicates = df[df.duplicated()]
print(duplicates)

"""# 2. Cleaning steps. Example:

a. Convert prices to numeric
"""

df["Episodes"] = pd.to_numeric(df["Episodes"], errors="coerce")  # '?' станет NaN
print("\nПропуски после конвертации Episodes в число:")
print(df["Episodes"].isnull().sum())

df["Members"] = df["Members"].str.replace(',', '').str.replace(' members', '').astype(float)

df.loc[:, 'Start_Date'] = pd.to_datetime(df['Start_Date'], format='%b %Y', errors='coerce')
df.loc[:, 'End_Date']   = pd.to_datetime(df['End_Date'],   format='%b %Y', errors='coerce') # а здесь ongoig or sep - станут NaT

"""b. Clean text fields (strip, lowercase, remove HTML tags if needed)

"""

from bs4 import BeautifulSoup

def clean_text(text):
    if pd.isna(text): # если значение в ячейке пустое (NaN), функция сразу возвращает его, чтобы не ломать обработку
        return text
    text = text.strip().lower()               # пробелы + нижний регистр
    text = BeautifulSoup(text, "lxml").get_text()  # удалить HTML
    text = text.replace('\n', ' ').replace('\t', ' ').replace('  ', ' ') # удаляет перенос строки и табуляцию если они есть но в целом их нету
    return text

df['Title'] = df['Title'].apply(clean_text)

"""с. Handle missing values"""

df_clean = df.copy()

# --- 1. Приводим числовые поля к числу ---
df_clean['Score'] = df_clean['Score'].fillna(df_clean['Score'].mean())
df_clean['Members'] = df_clean['Members'].str.replace(',', '').str.replace(' members', '').astype(float)
df_clean['Episodes'] = pd.to_numeric(df_clean['Episodes'], errors='coerce').fillna(12)

"""d. Remove outliers"""

Q1 = df['Members'].quantile(0.25)
Q3 = df['Members'].quantile(0.75)
IQR = Q3 - Q1 # this is Inter quartile range

df = df[(df['Members'] >= Q1 - 1.5*IQR) & (df['Members'] <= Q3 + 1.5*IQR)]

"""e. Create additional columns (derived features)"""

df_clean['Start_Date'] = pd.to_datetime(df_clean['Start_Date'], format='%b %Y', errors='coerce')
df_clean['End_Date']   = pd.to_datetime(df_clean['End_Date'], format='%b %Y', errors='coerce')


# Вывести строки с некорректными датами
invalid_start = df[df['Start_Date'].isna()]
invalid_end = df[df['End_Date'].isna()]

print("Некорректные Start_Date:")
print(invalid_start[['Title', 'Start_Date']])

print("\nНекорректные End_Date:")
print(invalid_end[['Title', 'End_Date']])

today = pd.Timestamp.today()
df_clean.loc[df_clean['End_Date'].isna(), 'End_Date'] = today

# Длительность аниме в месяцах (если обе даты известны)
df_clean['Duration_months'] = (
    (df_clean['End_Date'].dt.year - df_clean['Start_Date'].dt.year) * 12 +
    (df_clean['End_Date'].dt.month - df_clean['Start_Date'].dt.month)
)

# Категория по Score
df_clean['Score_group'] = pd.cut(
    df_clean['Score'],
    bins=[0, 7, 8, 9, 10],
    labels=['Low', 'Medium', 'High', 'Top']
)
df_clean

"""
# Part 3 — Exploratory Data Analysis (EDA)"""

print("Descriptive statistics for numerical columns of df:")
display(df.describe())

print("\nDataFrame information (data types, non-null counts) of df:")
df.info()

"""### Duration Months vs. Type

This box plot displays the distribution of 'Duration_months' for each 'Type' of anime. It helps in understanding how the length of anime varies across different formats.
"""

plt.figure(figsize=(12, 7))
sns.boxplot(data=df_clean, x='Type', y='Duration_months', palette='viridis', hue='Type', legend=False)
plt.title('Distribution of Anime Duration (Months) by Type')
plt.xlabel('Anime Type')
plt.ylabel('Duration (Months)')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

"""### Members vs. Duration Months

This scatter plot explores the relationship between the number of 'Members' and the 'Duration_months' of an anime, helping to identify any trends or correlations.
"""

plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_clean, x='Duration_months', y='Members', alpha=0.6, hue='Score_group', palette='coolwarm')
plt.title('Members vs. Duration Months')
plt.xlabel('Duration (Months)')
plt.ylabel('Number of Members')
plt.grid(True)
plt.tight_layout()
plt.show()

"""### Seasonal Patterns: When Most Anime are Released

These bar charts visualize the release patterns of anime by month and by year, allowing us to identify seasonal trends and periods with higher release volumes.
"""

# Extract month and year from Start_Date
df_clean['Start_Month'] = df_clean['Start_Date'].dt.month_name()
df_clean['Start_Year'] = df_clean['Start_Date'].dt.year

# Order of months for plotting
month_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']

plt.figure(figsize=(15, 6))

# Plot 1: Anime releases by month
plt.subplot(1, 2, 1)
sns.countplot(data=df_clean, x='Start_Month', order=month_order, palette='mako', hue='Start_Month', legend=False)
plt.title('Anime Releases by Month')
plt.xlabel('Month')
plt.ylabel('Number of Anime Released')
plt.xticks(rotation=45, ha='right')

# Plot 2: Anime releases by year (top 10 years)
plt.subplot(1, 2, 2)
top_years = df_clean['Start_Year'].value_counts().nlargest(10).index
sns.countplot(data=df_clean[df_clean['Start_Year'].isin(top_years)], x='Start_Year', order=top_years, palette='rocket', hue='Start_Year', legend=False)
plt.title('Top 10 Anime Release Years')
plt.xlabel('Year')
plt.ylabel('Number of Anime Released')
plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
sns.countplot(data=df_clean, x='Type', order=df_clean['Type'].value_counts().index, palette='crest')
plt.title('Distribution of Anime Types')
plt.xlabel('Anime Type')
plt.ylabel('Count')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1) # Histogram for raw scores
sns.histplot(df_clean['Score'], kde=True, bins=20, palette='viridis')
plt.title('Distribution of Anime Scores')
plt.xlabel('Score')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2) # Bar plot for score groups
sns.countplot(data=df_clean, x='Score_group', order=['Low', 'Medium', 'High', 'Top'], palette='rocket')
plt.title('Distribution of Anime Score Groups')
plt.xlabel('Score Group')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

numerical_cols = ['Score', 'Episodes', 'Members', 'Duration_months']
correlation_matrix = df_clean[numerical_cols].corr()

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Matrix of Numerical Features')
plt.show()

plt.figure(figsize=(10, 6))
sns.histplot(df_clean['Episodes'], bins=30, kde=True, palette='viridis')
plt.title('Distribution of Anime Episodes')
plt.xlabel('Number of Episodes')
plt.ylabel('Frequency')
plt.tight_layout()
plt.show()

"""# Task
The previous code block failed with a `TypeError` because the 'Score' column, after being loaded from the CSV or scraped, was not properly converted to a numeric type before attempting to calculate its mean. The error message indicated a long concatenated string of numbers, which implies that the column was treated as an object type containing strings rather than numbers.

To fix this, I need to explicitly convert the 'Score' column to a numeric type (float) using `pd.to_numeric` with `errors='coerce'` to handle any non-numeric values by turning them into `NaN`. This conversion should happen early in the data cleaning process, after `df` is loaded or created, and before any aggregation functions like `mean()` are called on it.

I will regenerate the last code cell, ensuring that the 'Score' column is correctly converted to numeric before any further operations.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests
from bs4 import BeautifulSoup
import time
import os

# Define the filename for the scraped data
filename = "Anime_top_10000.csv"

# Check if the CSV file exists. If not, scrape a small portion to create it.
if os.path.exists(filename):
    print(f"Loading data from existing CSV file: '{filename}'")
    df = pd.read_csv(filename)
else:
    print(f"CSV file '{filename}' not found. Performing minimal scraping to generate data.")
    titles = []
    scores = []
    anime_type_list = []
    episodes_list = []
    start_date_list = []
    end_date_list = []
    members_list = []
    headers = {"User-Agent": "Mozilla/5.0"}

    # Perform a very limited scrape (e.g., just the first page) to create a valid CSV
    # This is a pragmatic fix to resolve FileNotFoundError without full-scale scraping here.
    # For a full dataset, the user would need to execute the original scraping cell.
    for limit in range(0, 50, 50): # Scrapes only the first 50 entries
        url = f"https://myanimelist.net/topanime.php?limit={limit}&ajax=1"
        r = requests.get(url, headers=headers)
        soup = BeautifulSoup(r.text, "html.parser")
        rows = soup.find_all("tr", class_="ranking-list")

        if not rows:
            print("No data rows found during minimal scrape. Exiting scraping process.")
            break

        for row in rows:
            title = row.find("h3").text.strip()
            titles.append(title)
            score_tag = row.select_one("td.score span")
            scores.append(score_tag.text.strip() if score_tag else "N/A")
            info = row.find("div", class_="information")
            if info:
                lines = [l.strip() for l in info.text.split("\n") if l.strip()]
            else:
                lines = ["N/A", "N/A", "N/A"]

            type_ep = lines[0] if len(lines) > 0 else "N/A"
            if "(" in type_ep:
                anime_type = type_ep.split("(")[0].strip()
                eps = type_ep.split("(")[1].replace("eps)", "").replace("ep)", "").strip()
            else:
                anime_type = type_ep
                eps = "N/A"
            anime_type_list.append(anime_type)
            episodes_list.append(eps)

            date_line = lines[1] if len(lines) > 1 else "N/A"
            date_parts = [part.strip() for part in date_line.split(" - ")]
            current_start_date = "N/A"
            current_end_date = "N/A"
            if len(date_parts) == 2:
                current_start_date = date_parts[0]
                current_end_date = date_parts[1]
            elif len(date_parts) == 1:
                current_start_date = date_parts[0]
                current_end_date = "Ongoing"
            start_date_list.append(current_start_date)
            end_date_list.append(current_end_date)

            members_line = lines[2] if len(lines) > 2 else "N/A"
            members_list.append(members_line)

    df = pd.DataFrame({
        "Title": titles,
        "Score": scores,
        "Type": anime_type_list,
        "Episodes": episodes_list,
        "Start_Date": start_date_list,
        "End_Date": end_date_list,
        "Members": members_list
    })
    df.to_csv(filename, index=False, encoding="utf-8-sig")
    print(f"Minimal data scraped and saved ({len(df)} records) to '{filename}'.")

# --- Cleaning and Feature Engineering (as per original notebook steps) ---

# CRITICAL FIX: Convert 'Score' to numeric early
df['Score'] = pd.to_numeric(df['Score'], errors='coerce')

# Apply initial cleaning and type conversions to df
df["Episodes"] = pd.to_numeric(df["Episodes"], errors="coerce")
df["Members"] = df["Members"].astype(str).str.replace(',', '').str.replace(' members', '').astype(float)
df.loc[:, 'Start_Date'] = pd.to_datetime(df['Start_Date'], format='%b %Y', errors='coerce')
df.loc[:, 'End_Date'] = pd.to_datetime(df['End_Date'], format='%b %Y', errors='coerce')

def clean_text(text):
    if pd.isna(text):
        return text
    text = text.strip().lower()
    text = BeautifulSoup(text, "lxml").get_text()
    text = text.replace('\n', ' ').replace('\t', ' ').replace('  ', ' ')
    return text
df['Title'] = df['Title'].apply(clean_text)

# Handle outliers in 'Members'
Q1 = df['Members'].quantile(0.25)
Q3 = df['Members'].quantile(0.75)
IQR = Q3 - Q1
df = df[(df['Members'] >= Q1 - 1.5*IQR) & (df['Members'] <= Q3 + 1.5*IQR)]

# Create df_clean and apply further cleaning/feature engineering
df_clean = df.copy()

df_clean['Score'] = df_clean['Score'].fillna(df_clean['Score'].mean())
# Fill missing 'Episodes' (NaNs from '?' conversion) with 12
df_clean['Episodes'] = df_clean['Episodes'].fillna(12)

# Fill missing 'End_Date' with today's date
today = pd.Timestamp.today()
df_clean.loc[df_clean['End_Date'].isna(), 'End_Date'] = today

# Calculate 'Duration_months'
df_clean['Duration_months'] = (
    (df_clean['End_Date'].dt.year - df_clean['Start_Date'].dt.year) * 12 +
    (df_clean['End_Date'].dt.month - df_clean['Start_Date'].dt.month)
)

# Create 'Score_group' category
df_clean['Score_group'] = pd.cut(
    df_clean['Score'],
    bins=[0, 7, 8, 9, 10],
    labels=['Low', 'Medium', 'High', 'Top']
)

# --- Create the scatter plot ---
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_clean, x='Episodes', y='Score', alpha=0.6)

plt.xlabel('Number of Episodes')
plt.ylabel('Score')
plt.title('Relationship between Episodes and Score')

plt.grid(True)
plt.show()

```

## Visualize Episodes vs Score

### Subtask:
Create a scatter plot to visualize the relationship between 'Episodes' and 'Score' in the `df_clean` DataFrame. Include appropriate labels for axes and a title for the plot.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests
from bs4 import BeautifulSoup
import time
import os

# Define the filename for the scraped data
filename = "Anime_top_10000.csv"

# Check if the CSV file exists. If not, scrape a small portion to create it.
if os.path.exists(filename):
    print(f"Loading data from existing CSV file: '{filename}'")
    df = pd.read_csv(filename)
else:
    print(f"CSV file '{filename}' not found. Performing minimal scraping to generate data.")
    titles = []
    scores = []
    anime_type_list = []
    episodes_list = []
    start_date_list = []
    end_date_list = []
    members_list = []
    headers = {"User-Agent": "Mozilla/5.0"}

    # Perform a very limited scrape (e.g., just the first page) to create a valid CSV
    for limit in range(0, 50, 50): # Scrapes only the first 50 entries
        url = f"https://myanimelist.net/topanime.php?limit={limit}&ajax=1"
        r = requests.get(url, headers=headers)
        soup = BeautifulSoup(r.text, "html.parser")
        rows = soup.find_all("tr", class_="ranking-list")

        if not rows:
            print("No data rows found during minimal scrape. Exiting scraping process.")
            break

        for row in rows:
            title = row.find("h3").text.strip()
            titles.append(title)
            score_tag = row.select_one("td.score span")
            scores.append(score_tag.text.strip() if score_tag else "N/A")
            info = row.find("div", class_="information")
            if info:
                lines = [l.strip() for l in info.text.split("\n") if l.strip()]
            else:
                lines = ["N/A", "N/A", "N/A"]

            type_ep = lines[0] if len(lines) > 0 else "N/A"
            if "(" in type_ep:
                anime_type = type_ep.split("(")[0].strip()
                eps = type_ep.split("(")[1].replace("eps)", "").replace("ep)", "").strip()
            else:
                anime_type = type_ep
                eps = "N/A"
            anime_type_list.append(anime_type)
            episodes_list.append(eps)

            date_line = lines[1] if len(lines) > 1 else "N/A"
            date_parts = [part.strip() for part in date_line.split(" - ")]
            current_start_date = "N/A"
            current_end_date = "N/A"
            if len(date_parts) == 2:
                current_start_date = date_parts[0]
                current_end_date = date_parts[1]
            elif len(date_parts) == 1:
                current_start_date = date_parts[0]
                current_end_date = "Ongoing"
            start_date_list.append(current_start_date)
            end_date_list.append(current_end_date)

            members_line = lines[2] if len(lines) > 2 else "N/A"
            members_list.append(members_line)

    df = pd.DataFrame({
        "Title": titles,
        "Score": scores,
        "Type": anime_type_list,
        "Episodes": episodes_list,
        "Start_Date": start_date_list,
        "End_Date": end_date_list,
        "Members": members_list
    })
    df.to_csv(filename, index=False, encoding="utf-8-sig")
    print(f"Minimal data scraped and saved ({len(df)} records) to '{filename}'.")

# --- Cleaning and Feature Engineering (as per original notebook steps) ---

# Convert 'Score' to numeric early
df['Score'] = pd.to_numeric(df['Score'], errors='coerce')

# Apply initial cleaning and type conversions to df
df["Episodes"] = pd.to_numeric(df["Episodes"], errors="coerce")
df["Members"] = df["Members"].astype(str).str.replace(',', '').str.replace(' members', '').astype(float)
df.loc[:, 'Start_Date'] = pd.to_datetime(df['Start_Date'], format='%b %Y', errors='coerce')
df.loc[:, 'End_Date'] = pd.to_datetime(df['End_Date'], format='%b %Y', errors='coerce')

def clean_text(text):
    if pd.isna(text):
        return text
    text = text.strip().lower()
    text = BeautifulSoup(text, "lxml").get_text()
    text = text.replace('\n', ' ').replace('\t', ' ').replace('  ', ' ')
    return text
df['Title'] = df['Title'].apply(clean_text)

# Handle outliers in 'Members'
Q1 = df['Members'].quantile(0.25)
Q3 = df['Members'].quantile(0.75)
IQR = Q3 - Q1
df = df[(df['Members'] >= Q1 - 1.5*IQR) & (df['Members'] <= Q3 + 1.5*IQR)]

# Create df_clean and apply further cleaning/feature engineering
df_clean = df.copy()

df_clean['Score'] = df_clean['Score'].fillna(df_clean['Score'].mean())
# Fill missing 'Episodes' (NaNs from '?' conversion) with 12
df_clean['Episodes'] = df_clean['Episodes'].fillna(12)

# Ensure 'Start_Date' and 'End_Date' are datetime objects in df_clean
df_clean['Start_Date'] = pd.to_datetime(df_clean['Start_Date'], errors='coerce')
df_clean['End_Date'] = pd.to_datetime(df_clean['End_Date'], errors='coerce')

# Fill missing 'End_Date' with today's date
today = pd.Timestamp.today()
df_clean.loc[df_clean['End_Date'].isna(), 'End_Date'] = today

# Calculate 'Duration_months'
df_clean['Duration_months'] = (
    (df_clean['End_Date'].dt.year - df_clean['Start_Date'].dt.year) * 12 +
    (df_clean['End_Date'].dt.month - df_clean['Start_Date'].dt.month)
)

# Create 'Score_group' category
df_clean['Score_group'] = pd.cut(
    df_clean['Score'],
    bins=[0, 7, 8, 9, 10],
    labels=['Low', 'Medium', 'High', 'Top']
)

# --- Create the scatter plot ---
plt.figure(figsize=(10, 6))
sns.scatterplot(data=df_clean, x='Episodes', y='Score', alpha=0.6)

plt.xlabel('Number of Episodes')
plt.ylabel('Score')
plt.title('Relationship between Episodes and Score')

plt.grid(True)
plt.show()

"""## Visualize Type Distribution

### Subtask:
Generate a bar chart to display the distribution of anime 'Type' in the `df_clean` DataFrame, showing the count of each type. Ensure the chart has clear labels and a title.

**Reasoning**:
I need to generate a bar chart to visualize the distribution of anime 'Type' from the `df_clean` DataFrame. I will use `seaborn.countplot` for this, and then add appropriate labels and a title using `matplotlib.pyplot`.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Create a bar chart for the distribution of 'Type'
plt.figure(figsize=(12, 6))
sns.countplot(data=df_clean, x='Type', order=df_clean['Type'].value_counts().index, palette='viridis')

# Add labels and title
plt.xlabel('Anime Type')
plt.ylabel('Count')
plt.title('Distribution of Anime Types')

# Rotate x-axis labels if they overlap
plt.xticks(rotation=45, ha='right')

# Display the plot
plt.tight_layout()
plt.show()

"""**Reasoning**:
The previous code block successfully generated the plot but also raised a `FutureWarning` from seaborn regarding the `palette` usage with `countplot`. I will modify the `sns.countplot` call to follow the suggested best practice from the warning, by adding `hue='Type'` and `legend=False`, to suppress the warning and ensure future compatibility.


"""

import matplotlib.pyplot as plt
import seaborn as sns

# Create a bar chart for the distribution of 'Type'
plt.figure(figsize=(12, 6))
sns.countplot(data=df_clean, x='Type', order=df_clean['Type'].value_counts().index, palette='viridis', hue='Type', legend=False)

# Add labels and title
plt.xlabel('Anime Type')
plt.ylabel('Count')
plt.title('Distribution of Anime Types')

# Rotate x-axis labels if they overlap
plt.xticks(rotation=45, ha='right')

# Display the plot
plt.tight_layout()
plt.show()

"""## Visualize Score Group vs Type Distribution

### Subtask:
Create a visualization (e.g., a grouped bar chart or a stacked bar chart) to show the relationship between 'Score_group' and 'Type' from the `df_clean` DataFrame. This will illustrate how different anime types are distributed across the defined score groups. Include appropriate labels, a title, and a legend for the plot.

**Reasoning**:
To visualize the relationship between 'Score_group' and 'Type', I will use `seaborn.countplot` to create a grouped bar chart as instructed, setting 'Type' on the x-axis and 'Score_group' as the hue.
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Create a grouped bar chart to show the distribution of anime types by score group
plt.figure(figsize=(14, 7))
sns.countplot(data=df_clean, x='Type', hue='Score_group', palette='viridis')

# Add labels and title
plt.xlabel('Anime Type')
plt.ylabel('Count')
plt.title('Distribution of Anime Types by Score Group')

# Rotate x-axis labels for better readability
plt.xticks(rotation=45, ha='right')

# Add a legend
plt.legend(title='Score Group')

# Adjust layout and display the plot
plt.tight_layout()
plt.show()

